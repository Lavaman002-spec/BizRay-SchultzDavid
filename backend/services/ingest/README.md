# üöÄ Bulk API Ingestion - Complete Solution

## üìã Summary

You now have a complete bulk ingestion system that fetches company data from the Austrian Business Register API (`WSDL_URL`) and stores it in your PostgreSQL database (Docker, port 5432:5432).

## ‚úÖ What's Ready

### Core Components

- ‚úÖ **Bulk Ingestion Module** (`etl/bulk_api_ingest.py`)
- ‚úÖ **CLI Interface** (`run_bulk_ingest.py`)
- ‚úÖ **Shell Wrapper** (`bulk_ingest.sh`)
- ‚úÖ **Examples** (`examples.py`)

### Documentation

- ‚úÖ **Complete Guide** (`BULK_INGESTION_GUIDE.md`)
- ‚úÖ **Quick Start** (`QUICKSTART.md`)
- ‚úÖ **Implementation Summary** (`IMPLEMENTATION_SUMMARY.md`)

### Database

- ‚úÖ PostgreSQL running on port 5432:5432
- ‚úÖ Schema with staging and canonical tables
- ‚úÖ Indexes for fast searching

## üéØ How to Use (3 Ways)

### Option 1: Shell Script (Easiest) ‚≠ê

```bash
cd backend/services/ingest
./bulk_ingest.sh --max 100
```

### Option 2: Python CLI

```bash
cd backend/services/ingest
python run_bulk_ingest.py --max 100 --delay 1.0
```

### Option 3: Python Code

```python
from etl.bulk_api_ingest import bulk_ingest_from_api
from etl.normalize import run_normalization

stats = bulk_ingest_from_api(max_companies=100)
run_normalization()
```

## üìä Quick Reference

| Task                       | Command                       | Time   |
| -------------------------- | ----------------------------- | ------ |
| **Test (10 companies)**    | `./bulk_ingest.sh --max 10`   | ~10s   |
| **Small (50 companies)**   | `./bulk_ingest.sh --max 50`   | ~1min  |
| **Medium (100 companies)** | `./bulk_ingest.sh --max 100`  | ~2min  |
| **Large (500 companies)**  | `./bulk_ingest.sh --max 500`  | ~12min |
| **Bulk (1000 companies)**  | `./bulk_ingest.sh --max 1000` | ~25min |

## üîß Configuration

Your `.env` file is already configured:

```properties
WSDL_URL=https://justizonline.gv.at/jop/api/at.gv.justiz.fbw/ws/fbw.wsdl
API_KEY=210c4b02-44bf-4a70-aa3f-13b4d98a00d5
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=bizray
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
```

## üé¨ Getting Started

### 1. Verify Prerequisites

```bash
# Check Docker
docker ps | grep postgres
# Should show: bizray-db running on 5432:5432

# Test database connection
docker exec bizray-db psql -U postgres -d bizray -c "SELECT 1"
```

### 2. Run Your First Ingestion

```bash
cd backend/services/ingest

# Small test (10 companies)
python run_bulk_ingest.py --max 10 --delay 0.5
```

### 3. Check Results

```bash
# In SQL
docker exec bizray-db psql -U postgres -d bizray -c "
  SELECT COUNT(*) as companies FROM companies;
  SELECT COUNT(*) as officers FROM officers;
"

# Or using Python
cd backend/services/ingest
python examples.py
# Choose option 7 to query results
```

## üìà Data Flow

```
Austrian Business Register API
         ‚Üì
   [Discovery Phase]
    Search cities, legal forms, FNR ranges
         ‚Üì
   [Fetching Phase]
    Get detailed company data (AUSZUG_V2_)
         ‚Üì
   [Staging Tables]
    stg_companies, stg_officers, stg_links
    (Raw JSONB data preserved)
         ‚Üì
   [Normalization]
    Deduplicate, assign UUIDs, link relationships
         ‚Üì
   [Canonical Tables]
    companies, officers, ownership_links
    (Production-ready data)
         ‚Üì
   [Your Application]
    API queries, frontend display
```

## üõ†Ô∏è Common Tasks

### Fetch Specific Companies

```bash
python run_bulk_ingest.py --companies FN348406 FN10001 FN10002
```

### Large Ingestion

```bash
# More companies, slower rate (safer)
python run_bulk_ingest.py --max 1000 --delay 1.5
```

### Skip Discovery (Faster)

```bash
# Use sample companies instead of searching
python run_bulk_ingest.py --max 50 --no-discovery
```

### Only Stage (No Normalization)

```bash
# Useful for debugging
python run_bulk_ingest.py --max 100 --skip-normalize
```

## üìä Monitor Progress

The ingestion provides real-time updates:

```
[15/100] Fetching FN10015...
  ‚úÖ Success: Example Company GmbH

üìä Progress Checkpoint:
   Completed: 15/100 (15.0%)
   Success: 14, Failed: 1, Skipped: 0
   Rate: 0.83 companies/second
   ETA: 1.7 minutes
```

## üîç Verify Data

### Check Database

```sql
-- Connect
docker exec -it bizray-db psql -U postgres -d bizray

-- Check counts
SELECT 'Companies' as table, COUNT(*) as count FROM companies
UNION ALL
SELECT 'Officers', COUNT(*) FROM officers
UNION ALL
SELECT 'Links', COUNT(*) FROM ownership_links;

-- View recent data
SELECT register_id, name, city, created_at
FROM companies
ORDER BY created_at DESC
LIMIT 10;

-- Search companies
SELECT register_id, name, city
FROM companies
WHERE name ILIKE '%company%'
LIMIT 10;
```

### Check via API

```bash
# Start API service
docker-compose up -d api_service

# Query companies
curl "http://localhost:8080/api/companies/search?q=company"
```

## üìö Documentation

- **`QUICKSTART.md`** - Quick reference for common commands
- **`BULK_INGESTION_GUIDE.md`** - Comprehensive guide (400+ lines)
- **`IMPLEMENTATION_SUMMARY.md`** - Technical implementation details
- **`examples.py`** - Interactive examples you can run

## üêõ Troubleshooting

### Database Connection Failed

```bash
# Restart database
docker-compose restart db

# Check logs
docker logs bizray-db

# Test connection
docker exec bizray-db psql -U postgres -d bizray -c "SELECT version()"
```

### API Authentication Error

```bash
# Check API key
cat .env | grep API_KEY

# Test API
cd backend/services/ingest
python -c "from etl.client import create_client; create_client(); print('OK')"
```

### Schema Missing

```bash
# Apply schema manually
docker exec bizray-db psql -U postgres -d bizray -f /docker-entrypoint-initdb.d/001_init.sql

# Or via Python
cd backend/services/ingest
python -c "from etl.db import ensure_schema; ensure_schema()"
```

## üéì Learn More

### Try the Examples

```bash
cd backend/services/ingest
python examples.py
```

Interactive menu with 7 examples:

1. Basic bulk ingestion
2. Fetch specific companies
3. Fast ingestion (testing)
4. Custom discovery strategy
5. Incremental ingestion
6. Batch processing
7. Query results

### Read the Guides

- Start with `QUICKSTART.md`
- Deep dive with `BULK_INGESTION_GUIDE.md`
- Technical details in `IMPLEMENTATION_SUMMARY.md`

## üö¶ Next Steps

### 1. Run a Test Ingestion

```bash
cd backend/services/ingest
./bulk_ingest.sh --max 10
```

### 2. Start the API

```bash
cd ../../../  # Back to project root
docker-compose up -d api_service
```

### 3. Query the Data

```bash
curl "http://localhost:8080/api/companies/search?q=company"
```

### 4. Start the Frontend

```bash
cd frontend
npm install
npm run dev
```

### 5. Browse the Data

Open http://localhost:3000 and explore!

## üí° Tips

### Rate Limiting

- Start with `--delay 1.0` (safe default)
- Increase to `1.5` or `2.0` for large batches
- Decrease to `0.5` only for small tests

### Resume on Failure

- System automatically skips already ingested companies
- Safe to stop and restart at any time
- No duplicate data

### Monitor Resources

```bash
# Check Docker resource usage
docker stats

# Check database size
docker exec bizray-db psql -U postgres -d bizray -c "
  SELECT pg_size_pretty(pg_database_size('bizray'))
"
```

## üìû Support

If you encounter issues:

1. Check the troubleshooting section above
2. Review logs: `docker-compose logs -f`
3. Verify configuration in `.env`
4. Consult the documentation files

## üéâ You're Ready!

Everything is set up and ready to use. Start with a small test:

```bash
cd backend/services/ingest
./bulk_ingest.sh --max 10
```

Good luck with your bulk ingestion! üöÄ

---

**Version**: 1.0  
**Status**: Production Ready ‚úÖ  
**Database**: PostgreSQL 16 on port 5432:5432  
**API**: Austrian Business Register (justizonline.gv.at)
